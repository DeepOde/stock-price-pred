{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7de31f413531e91ebec935718bb5d86c720cbf6e34786de01ce87ffc441a6e95"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view as sliding_window_view\n",
    "import pickle\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSEDataset(Dataset):\n",
    "    def __init__(self, ohlcv_dir, target_ticker, target_ticker_file, len_window, len_corr_traceback, nP, nN, \n",
    "    keep_tickers=None, ohlcv_prefix='', ohlcv_sufix='', ohlcv_files=None, start_date=None, end_date=None,\n",
    "    target_feat='c', keep_feat='ohlcva'):\n",
    "\n",
    "        feat_name_map = {\n",
    "            'o' : 'Open', \n",
    "            'h' : 'High', \n",
    "            'l' : 'Low', \n",
    "            'c' : 'Close', \n",
    "            'v' : 'Volume',\n",
    "            'a' : 'Adj Close'\n",
    "        }\n",
    "\n",
    "        self.len_window = len_window\n",
    "        self.len_corr_traceback = len_corr_traceback\n",
    "        self.nP, self.nN = nP, nN\n",
    "        self.target_feat = target_feat\n",
    "        self.keep_feat = keep_feat\n",
    "        self.start_date, self.end_date = start_date, end_date\n",
    "\n",
    "        if ohlcv_files is not None:\n",
    "            ohlcv_files = set(ohlcv_files)\n",
    "        \n",
    "        if keep_tickers is not None:\n",
    "            keep_tickers = set(keep_tickers)\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(ohlcv_dir, target_ticker_file))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "        \n",
    "        if end_date is not None:\n",
    "            end_mask =  df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(df)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        df = df.iloc[i_start:i_end]\n",
    "        self.mainstream_df = df.loc[:, target_ticker]\n",
    "        self.df = df.drop(target_ticker, axis=1)\n",
    "        \n",
    "        if ohlcv_files is not None and target_ticker_file not in ohlcv_files:\n",
    "            self.df = pd.DataFrame(columns=df.columns)\n",
    "            \n",
    "        for f in os.listdir(ohlcv_dir):\n",
    "            if f.startswith(ohlcv_prefix) and f.endswith(ohlcv_sufix) and (ohlcv_files is None or f in ohlcv_files):\n",
    "                if f == target_ticker_file:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(os.path.join(ohlcv_dir, f))\n",
    "                    temp_df.reset_index(drop=True, inplace=True)\n",
    "                    temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "                    self.df = pd.merge(self.df.reset_index(), temp_df, on=['Date', 'Ticker'],\n",
    "                    how='inner', suffixes=('', '_y')).set_index(['Date', 'Ticker'])\n",
    "                    self.df.drop(self.df.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n",
    "                    \n",
    "        if keep_tickers is not None:\n",
    "            for c in self.df:\n",
    "                if c not in keep_tickers:\n",
    "                    self.df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "        self.df = self.df.pivot_table(index='Date', columns='Ticker')\n",
    "        \n",
    "        self.df.columns = self.df.columns.map('_'.join)\n",
    "        if isinstance(self.mainstream_df, pd.Series):\n",
    "            self.mainstream_df = pd.DataFrame(self.mainstream_df)\n",
    "        \n",
    "        self.mainstream_df = self.mainstream_df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.mainstream_df.columns = self.mainstream_df.columns.map('_'.join)\n",
    "\n",
    "        target_feat_name = \"{}_{}\".format(target_ticker, feat_name_map[target_feat])\n",
    "        \n",
    "        self.unshifted_target = self.mainstream_df.loc[:, target_feat_name]\n",
    "        self.target = self.unshifted_target.shift(periods=-1)\n",
    "        \n",
    "        # To account for absence of target for last row.\n",
    "        self.df = self.df.iloc[:-1, :]  \n",
    "        self.mainstream_df = self.mainstream_df.iloc[:-1, :]\n",
    "\n",
    "        drop_features = set(feat_name_map.keys()).difference({feat for feat in keep_feat})\n",
    "        for feat in drop_features:\n",
    "            self.df.drop(self.df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "            self.mainstream_df.drop(self.mainstream_df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "\n",
    "        # For i_end, data of [i_end - (len_corr_traceback) : i_end] (py notation)\n",
    "        # is needed to calculate correlation, basis on which data of \n",
    "        # [i_end - (len_window) : i_end] (py notation) must be sent.\n",
    "        # i_end is excluded, so last i_end should be len(self.df)\n",
    "        \n",
    "        self.swdf = []\n",
    "        for i_end in range(len_corr_traceback, len(self.df)+1):\n",
    "            if i_end % 50 == 0:\n",
    "                print(i_end, len(self.df))\n",
    "            self.swdf.append(self.get_high_corr(self.unshifted_target.iloc[i_end-len_corr_traceback:i_end], \n",
    "            self.df.iloc[i_end-len_corr_traceback:i_end, :], len_window, nP, nN))\n",
    "            \n",
    "        # self.swdf = np.array(self.swdf).reshape(len(self.swdf), self.len_window, -1)\n",
    "        self.swdf = np.array(self.swdf)\n",
    "        \n",
    "        # if earlier self.df.shape was (6(n+1), c), it should now be\n",
    "        # (n, c), mainstream_df.shape and index_data_df should be (n, 1) and swdf.shape\n",
    "        # should be (n-lct+1, lw*(nP+nN)).\n",
    "\n",
    "        # For index 0, \n",
    "        # swdf[0], mainstream_df[lct-lw : lct] flattend (py notation)\n",
    "        # index_data_df[lct-lw : lct] flattened (py notation) should be accessed.\n",
    "\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        # Correct. Continue from here. \n",
    "\n",
    "        # The index data used is for a single index.\n",
    "        self.index_data_df = pd.read_csv(\"data_collection/NIFTY 50.csv\")\n",
    "        self.index_data_df['Date'] = pd.to_datetime(self.index_data_df['Date'])\n",
    "        self.index_data_df.rename(columns={'SharesTraded' : 'Volume'}, inplace=True)\n",
    "        self.index_data_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  self.index_data_df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "\n",
    "        if end_date is not None:\n",
    "            end_mask =  self.index_data_df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(self.index_data_df)\n",
    "\n",
    "        self.index_data_df.reset_index(drop=True, inplace=True)\n",
    "        self.index_data_df.set_index(['Date'], inplace=True)\n",
    "        self.index_data_df = self.index_data_df.iloc[i_start:i_end]\n",
    "        self.index_data_df = pd.DataFrame(self.index_data_df.loc[:, 'Close'])\n",
    "        self.index_data_df = self.index_data_df.reset_index().merge(\n",
    "    self.df.reset_index()['Date'], how='inner', on='Date').set_index('Date')\n",
    "        \n",
    "\n",
    "    def get_high_corr(self, target: pd.Series, candidates: pd.DataFrame, len_window, nP, nN):\n",
    "        corr = candidates.corrwith(target)\n",
    "        p_best = corr.nlargest(nP)\n",
    "        n_best = corr.nsmallest(nN)\n",
    "        newrow = candidates.iloc[-len_window:, candidates.columns.get_indexer(p_best.index)].melt()['value'].tolist()\n",
    "        newrow.extend(candidates.iloc[-len_window:, candidates.columns.get_indexer(n_best.index)].melt()['value'].tolist())\n",
    "        return newrow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.swdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        \n",
    "        return (self.swdf[idx, :].reshape(dataset.len_window, -1), \n",
    "        self.mainstream_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(), \n",
    "        self.index_data_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy())\n",
    "\n",
    "\n",
    "def save_NSEDataset(dataset, opfile):\n",
    "    with open(opfile, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def load_NSEDataset(ipfile):\n",
    "    PICKLE_PROTOCOL = 4\n",
    "    with open(ipfile, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " class MiLSTM(nn.Module):\n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        self.p_size = input_sz \n",
    "        self.n_size = input_sz \n",
    "        self.index_size = input_sz\n",
    "        p_sz, n_sz, index_sz = self.p_size, self.n_size, self.index_size\n",
    "\n",
    "        #f_t\n",
    "        self.Wfh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wfy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bf = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #o_t\n",
    "        self.Woh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Woy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bo = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_t\n",
    "        self.Wch = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bc = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_pt\n",
    "        self.Wcph = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcpp = nn.Parameter(torch.Tensor(p_sz, hidden_sz))\n",
    "        self.bcp = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #c_nt\n",
    "        self.Wcnh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcnn = nn.Parameter(torch.Tensor(n_sz, hidden_sz))\n",
    "        self.bcn = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_it\n",
    "        self.Wcih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wcii = nn.Parameter(torch.Tensor(index_sz, hidden_sz))\n",
    "        self.bci = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #i_t\n",
    "        self.Wih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wiy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bi = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #i_pt\n",
    "        self.Wiph = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wipy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bip = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #c_nt\n",
    "        self.Winh = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Winy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bin = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        #c_it\n",
    "        self.Wiih = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.Wiiy = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.bii = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "\n",
    "        #attn\n",
    "        self.alpha_t = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_pt = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_nt = nn.Parameter(torch.Tensor(1))\n",
    "        self.alpha_it = nn.Parameter(torch.Tensor(1))\n",
    "        self.Wattn = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.ba = nn.Parameter(torch.Tensor(1))\n",
    "        self.bap = nn.Parameter(torch.Tensor(1))\n",
    "        self.ban = nn.Parameter(torch.Tensor(1))\n",
    "        self.bai = nn.Parameter(torch.Tensor(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "        #c_pt\n",
    "        nn.init.zeros_(self.Wcph)\n",
    "        nn.init.zeros_(self.Wcpp)\n",
    "        nn.init.zeros_(self.bcp)\n",
    "        \n",
    "        #c_nt\n",
    "        nn.init.zeros_(self.Wcnh)\n",
    "        nn.init.zeros_(self.Wcnn)\n",
    "        nn.init.zeros_(self.bcn)\n",
    "        \n",
    "        #c_it\n",
    "        nn.init.zeros_(self.Wcih)\n",
    "        nn.init.zeros_(self.Wcii)\n",
    "        nn.init.zeros_(self.bci)\n",
    "        \n",
    "\n",
    "    def forward(self, y_tilde, p_tilde, n_tilde, index_tilde, init_stats=None):\n",
    "        batch_size, win_len, _ = y_tilde.shape\n",
    "        hidden_seqs = []\n",
    "        cell_states = []\n",
    "\n",
    "        if init_stats is None:\n",
    "            h_t, cell_t = (torch.zeros(batch_size, self.hidden_size).to(y_tilde.device), \n",
    "                        torch.zeros(batch_size, self.hidden_size).to(y_tilde.device))\n",
    "        else:\n",
    "            h_t, cell_t = init_states \n",
    "\n",
    "        \n",
    "        for t in range(win_len):\n",
    "            y_t = y_tilde[:, t, :]\n",
    "            p_t = p_tilde[:, t, :]\n",
    "            n_t = n_tilde[:, t, :]\n",
    "            index_t = index_tilde[:, t, :]\n",
    "\n",
    "            f_t = torch.sigmoid(y_t @ self.Wfy + h_t @ self.Wfh + self.bf)\n",
    "            o_t = torch.sigmoid(y_t @ self.Woy + h_t @ self.Woh + self.bo)\n",
    "            c_t = torch.tanh(y_t @ self.Wcy + h_t @ self.Wch + self.bc)\n",
    "            c_pt = torch.tanh(p_t @ self.Wcpp + h_t @ self.Wcph + self.bcp)\n",
    "            c_nt = torch.tanh(n_t @ self.Wcnn + h_t @ self.Wcnh + self.bcn)\n",
    "            c_it = torch.tanh(index_t @ self.Wcii + h_t @ self.Wcph + self.bci)\n",
    "\n",
    "            i_t = torch.sigmoid(y_t @ self.Wiy + h_t @ self.Wih + self.bi)\n",
    "            i_pt = torch.sigmoid(y_t @ self.Wipy + h_t @ self.Wiph + self.bip)\n",
    "            i_nt = torch.sigmoid(y_t @ self.Winy + h_t @ self.Winh + self.bin)\n",
    "            i_it  = torch.sigmoid(y_t @ self.Wiiy + h_t @ self.Wiih + self.bii)\n",
    "\n",
    "            l_t = torch.mul(c_t, i_t)\n",
    "            l_pt = torch.mul(c_pt, i_pt)\n",
    "            l_nt = torch.mul(c_nt, i_nt)\n",
    "            l_it = torch.mul(c_it, i_it)\n",
    "           \n",
    "            u_t = torch.mul(l_t @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_pt = torch.mul(l_pt @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_nt = torch.mul(l_nt @ self.Wattn, cell_t).sum(dim=1)\n",
    "            u_it = torch.mul(l_it @ self.Wattn, cell_t).sum(dim=1)\n",
    "\n",
    "            alphas = torch.stack((u_t, u_pt, u_nt, u_it), dim=1)\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            probs = softmax(alphas)\n",
    "            alpha_t, alpha_pt, alpha_nt, alpha_it = probs[:, 0], probs[:, 1], probs[:, 2], probs[:, 3]\n",
    "            \n",
    "            L_t = self.alpha_t*l_t + self.alpha_pt*l_pt + self.alpha_nt*l_nt + self.alpha_it*l_it\n",
    "\n",
    "            cell_t = torch.mul(cell_t, f_t) + L_t\n",
    "            h_t = torch.mul(torch.tanh(cell_t), o_t)\n",
    "            \n",
    "            hidden_seqs.append(h_t)\n",
    "            cell_states.append(cell_t)\n",
    "\n",
    "        hidden_seqs = torch.stack(hidden_seqs)\n",
    "        hidden_seqs = hidden_seqs.transpose(0, 1).contiguous()\n",
    "        return hidden_seqs, (h_t, cell_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, keep_features, hidden_sz1, hidden_sz2, hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3, lstm1_layers=1, lstm1_dropout=0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.hidden_sz1 = hidden_sz1\n",
    "        self.hidden_sz2 = hidden_sz2\n",
    "        self.hidden_sz_lin1, self.hidden_sz_lin2, self.hidden_sz_lin3 = hidden_sz_lin1, hidden_sz_lin2, hidden_sz_lin3\n",
    "        self.lstm = nn.LSTM(input_size = len(keep_features),\n",
    "        hidden_size = hidden_sz1,\n",
    "        num_layers = lstm1_layers,\n",
    "        batch_first = True,\n",
    "        dropout = lstm1_dropout\n",
    "        )\n",
    "\n",
    "        self.milstm = MiLSTM(hidden_sz1, hidden_sz2)\n",
    "        self.self_attn_linear = nn.Linear(in_features=hidden_sz2, out_features=hidden_sz2, bias=True)\n",
    "        self.self_attn_v = nn.Parameter(torch.rand(hidden_sz2))\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(hidden_sz2, hidden_sz_lin1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sz_lin1, hidden_sz_lin2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sz_lin2, hidden_sz_lin3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, swdf, mainstream, index, nP, nN):\n",
    "        y_tilde, _ = self.lstm(mainstream)\n",
    "\n",
    "        Xps = []\n",
    "        for i_feature in range(nP):\n",
    "            Xpi, _ = self.lstm(swdf[:, :, i_feature].unsqueeze(2))\n",
    "            Xps.append(Xpi)\n",
    "        p_tilde = torch.stack(Xps).mean(axis=0)\n",
    "\n",
    "        Xns = []\n",
    "        for i_feature in range(nP, nP+nN):\n",
    "            Xni, _ = self.lstm(swdf[:, :,i_feature].unsqueeze(2))\n",
    "            Xns.append(Xni)\n",
    "        n_tilde = torch.stack(Xns).mean(axis=0)\n",
    "        \n",
    "        index_tilde, _ = self.lstm(index)\n",
    "        print(y_tilde.shape, p_tilde.shape, n_tilde.shape, index_tilde.shape)\n",
    "        \n",
    "        y_tilde_prime, _ = self.milstm(y_tilde, p_tilde, n_tilde, index_tilde)\n",
    "\n",
    "        js_ = self.self_attn_linear(y_tilde_prime)\n",
    "        js = torch.tanh(js_).matmul(self.self_attn_v)\n",
    "        betas = torch.softmax(js, dim=1)\n",
    "        y_ = torch.matmul(betas.unsqueeze(1), y_tilde_prime).squeeze()\n",
    "        y_hat = self.regressor(y_).squeeze()\n",
    "        return y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        swdf, mainstream, index = sample\n",
    "        out = model(swdf.float(), mainstream.float(), index.float(), dataloader.dataset.nP, dataloader.dataset.nN)\n",
    "        print(out.shape)\n",
    "        '''\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        '''\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    '''\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([512, 10, 64]) torch.Size([512, 10, 64]) torch.Size([512, 10, 64]) torch.Size([512, 10, 64])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 10, 64]) torch.Size([512, 10, 64]) torch.Size([512, 10, 64]) torch.Size([512, 10, 64])\n",
      "torch.Size([512])\n",
      "torch.Size([242, 10, 64]) torch.Size([242, 10, 64]) torch.Size([242, 10, 64]) torch.Size([242, 10, 64])\n",
      "torch.Size([242])\n"
     ]
    }
   ],
   "source": [
    "dataset = load_NSEDataset('data_collection/pickled_datasets/itc_Jan17_w10_t20_p10_n10_o.pkl')\n",
    "train_dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "keep_features = 'o'\n",
    "hidden_sz1 = 64\n",
    "hidden_sz2 = 64\n",
    "hidden_sz_lin1 = 64\n",
    "hidden_sz_lin2 = 32\n",
    "hidden_sz_lin3 = 1\n",
    "model = NeuralNetwork(keep_features=keep_features, hidden_sz1=hidden_sz1, hidden_sz2=hidden_sz2, \n",
    "hidden_sz_lin1=hidden_sz_lin1, hidden_sz_lin2=hidden_sz_lin2, hidden_sz_lin3=hidden_sz_lin3).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "train(train_dataloader, model, loss_fn, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "# X = torch.rand(1, 28, 28, device=device)\n",
    "# logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([512, 10, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "torch.rand(9, 512, 10, 64).mean(axis=0).shape"
   ]
  }
 ]
}