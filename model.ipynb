{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "7de31f413531e91ebec935718bb5d86c720cbf6e34786de01ce87ffc441a6e95"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view as sliding_window_view\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "class NSEDataset(Dataset):\n",
    "    def __init__(self, ohlcv_dir, target_ticker, target_ticker_file, len_window, len_corr_traceback, nP, nN, \n",
    "    keep_tickers=None, ohlcv_prefix='', ohlcv_sufix='', ohlcv_files=None, start_date=None, end_date=None,\n",
    "    target_feat='c', keep_feat='ohlcva'):\n",
    "\n",
    "        feat_name_map = {\n",
    "            'o' : 'Open', \n",
    "            'h' : 'High', \n",
    "            'l' : 'Low', \n",
    "            'c' : 'Close', \n",
    "            'v' : 'Volume',\n",
    "            'a' : 'Adj Close'\n",
    "        }\n",
    "\n",
    "        self.len_window = len_window\n",
    "        self.len_corr_traceback = len_corr_traceback\n",
    "        self.nP, self.nN = nP, nN\n",
    "        self.target_feat = target_feat\n",
    "        self.keep_feat = keep_feat\n",
    "        self.start_date, self.end_date = start_date, end_date\n",
    "\n",
    "        if ohlcv_files is not None:\n",
    "            ohlcv_files = set(ohlcv_files)\n",
    "        \n",
    "        if keep_tickers is not None:\n",
    "            keep_tickers = set(keep_tickers)\n",
    "    \n",
    "        df = pd.read_csv(os.path.join(ohlcv_dir, target_ticker_file))\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "        \n",
    "        if end_date is not None:\n",
    "            end_mask =  df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(df)\n",
    "\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        df.set_index(['Date', 'Ticker'], inplace=True)\n",
    "        df = df.iloc[i_start:i_end]\n",
    "        self.mainstream_df = df.loc[:, target_ticker]\n",
    "        self.df = df.drop(target_ticker, axis=1)\n",
    "        \n",
    "        if ohlcv_files is not None and target_ticker_file not in ohlcv_files:\n",
    "            self.df = pd.DataFrame(columns=df.columns)\n",
    "            \n",
    "        for f in os.listdir(ohlcv_dir):\n",
    "            if f.startswith(ohlcv_prefix) and f.endswith(ohlcv_sufix) and (ohlcv_files is None or f in ohlcv_files):\n",
    "                if f == target_ticker_file:\n",
    "                    continue\n",
    "                else:\n",
    "                    temp_df = pd.read_csv(os.path.join(ohlcv_dir, f))\n",
    "                    temp_df.reset_index(drop=True, inplace=True)\n",
    "                    temp_df['Date'] = pd.to_datetime(temp_df['Date'])\n",
    "                    self.df = pd.merge(self.df.reset_index(), temp_df, on=['Date', 'Ticker'],\n",
    "                    how='inner', suffixes=('', '_y')).set_index(['Date', 'Ticker'])\n",
    "                    self.df.drop(self.df.filter(regex='_y$').columns.tolist(), axis=1, inplace=True)\n",
    "                    \n",
    "        if keep_tickers is not None:\n",
    "            for c in self.df:\n",
    "                if c not in keep_tickers:\n",
    "                    self.df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "        self.df = self.df.pivot_table(index='Date', columns='Ticker')\n",
    "        \n",
    "        self.df.columns = self.df.columns.map('_'.join)\n",
    "        if isinstance(self.mainstream_df, pd.Series):\n",
    "            self.mainstream_df = pd.DataFrame(self.mainstream_df)\n",
    "        \n",
    "        self.mainstream_df = self.mainstream_df.pivot_table(index='Date', columns='Ticker')\n",
    "        self.mainstream_df.columns = self.mainstream_df.columns.map('_'.join)\n",
    "\n",
    "        target_feat_name = \"{}_{}\".format(target_ticker, feat_name_map[target_feat])\n",
    "        \n",
    "        self.unshifted_target = self.mainstream_df.loc[:, target_feat_name]\n",
    "        self.target = self.unshifted_target.shift(periods=-1)\n",
    "        \n",
    "        # To account for absence of target for last row.\n",
    "        self.df = self.df.iloc[:-1, :]  \n",
    "        self.mainstream_df = self.mainstream_df.iloc[:-1, :]\n",
    "\n",
    "        drop_features = set(feat_name_map.keys()).difference({feat for feat in keep_feat})\n",
    "        for feat in drop_features:\n",
    "            self.df.drop(self.df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "            self.mainstream_df.drop(self.mainstream_df.filter(regex='_{}$'.format(feat_name_map[feat])).columns.tolist(), axis=1, inplace=True)\n",
    "\n",
    "        # For i_end, data of [i_end - (len_corr_traceback) : i_end] (py notation)\n",
    "        # is needed to calculate correlation, basis on which data of \n",
    "        # [i_end - (len_window) : i_end] (py notation) must be sent.\n",
    "        # i_end is excluded, so last i_end should be len(self.df)\n",
    "        \n",
    "        self.swdf = []\n",
    "        for i_end in range(len_corr_traceback, len(self.df)+1):\n",
    "            if i_end % 50 == 0:\n",
    "                print(i_end, len(self.df))\n",
    "            self.swdf.append(self.get_high_corr(self.unshifted_target.iloc[i_end-len_corr_traceback:i_end], \n",
    "            self.df.iloc[i_end-len_corr_traceback:i_end, :], len_window, nP, nN))\n",
    "            \n",
    "        # self.swdf = np.array(self.swdf).reshape(len(self.swdf), self.len_window, -1)\n",
    "        self.swdf = np.array(self.swdf)\n",
    "        \n",
    "        # if earlier self.df.shape was (6(n+1), c), it should now be\n",
    "        # (n, c), mainstream_df.shape and index_data_df should be (n, 1) and swdf.shape\n",
    "        # should be (n-lct+1, lw*(nP+nN)).\n",
    "\n",
    "        # For index 0, \n",
    "        # swdf[0], mainstream_df[lct-lw : lct] flattend (py notation)\n",
    "        # index_data_df[lct-lw : lct] flattened (py notation) should be accessed.\n",
    "\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        # Correct. Continue from here. \n",
    "\n",
    "        # The index data used is for a single index.\n",
    "        self.index_data_df = pd.read_csv(\"data_collection/NIFTY 50.csv\")\n",
    "        self.index_data_df['Date'] = pd.to_datetime(self.index_data_df['Date'])\n",
    "        self.index_data_df.rename(columns={'SharesTraded' : 'Volume'}, inplace=True)\n",
    "        self.index_data_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "        if start_date is not None:\n",
    "            start_mask =  self.index_data_df['Date'] >= datetime.datetime.fromisoformat(start_date)\n",
    "            i_start = start_mask[start_mask].index.min()\n",
    "        else:\n",
    "            i_start = 0\n",
    "\n",
    "        if end_date is not None:\n",
    "            end_mask =  self.index_data_df['Date'] > datetime.datetime.fromisoformat(end_date)\n",
    "            i_end = end_mask[end_mask].index.min()\n",
    "        else:\n",
    "            i_end = len(self.index_data_df)\n",
    "\n",
    "        self.index_data_df.reset_index(drop=True, inplace=True)\n",
    "        self.index_data_df.set_index(['Date'], inplace=True)\n",
    "        self.index_data_df = self.index_data_df.iloc[i_start:i_end]\n",
    "        self.index_data_df = pd.DataFrame(self.index_data_df.loc[:, 'Close'])\n",
    "        self.index_data_df = self.index_data_df.reset_index().merge(\n",
    "    self.df.reset_index()['Date'], how='inner', on='Date').set_index('Date')\n",
    "        \n",
    "\n",
    "    def get_high_corr(self, target: pd.Series, candidates: pd.DataFrame, len_window, nP, nN):\n",
    "        corr = candidates.corrwith(target)\n",
    "        p_best = corr.nlargest(nP)\n",
    "        n_best = corr.nsmallest(nN)\n",
    "        newrow = candidates.iloc[-len_window:, candidates.columns.get_indexer(p_best.index)].melt()['value'].tolist()\n",
    "        newrow.extend(candidates.iloc[-len_window:, candidates.columns.get_indexer(n_best.index)].melt()['value'].tolist())\n",
    "        return newrow\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.swdf)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For index i < len(swdf), \n",
    "        # swdf[i], mainstream_df[lct-lw + i: lct + i] flattend (py notation)\n",
    "        # index_data_df[lct-lw + i : lct + i] flattened (py notation) should be accessed.\n",
    "        \n",
    "        return (self.swdf[idx, :].reshape(dataset.len_window, -1), \n",
    "        self.mainstream_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy(), \n",
    "        self.index_data_df.iloc[self.len_corr_traceback-self.len_window+idx : self.len_corr_traceback+idx].to_numpy())\n",
    "\n",
    "\n",
    "def save_NSEDataset(dataset, opfile):\n",
    "    with open(opfile, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "def load_NSEDataset(ipfile):\n",
    "    PICKLE_PROTOCOL = 4\n",
    "    with open(ipfile, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, keep_features, hidden_sz, lstm1_layers=1, lstm1_dropout=0):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.LSTM = nn.LSTM(input_size = len(keep_features),\n",
    "        hidden_size = hidden_sz,\n",
    "        num_layers = lstm1_layers,\n",
    "        batch_first = True,\n",
    "        dropout = lstm1_dropout\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hncn = self.LSTM(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        print(i, sample[0].shape, sample[1].shape, sample[2].shape)\n",
    "        swdf, mainstream, index = sample\n",
    "        out = model(mainstream.float())\n",
    "        print(out.shape)\n",
    "        '''\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        '''\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    '''\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 torch.Size([512, 10, 20]) torch.Size([512, 10, 1]) torch.Size([512, 10, 1])\ntorch.Size([512, 10, 64])\n1 torch.Size([512, 10, 20]) torch.Size([512, 10, 1]) torch.Size([512, 10, 1])\ntorch.Size([512, 10, 64])\n2 torch.Size([242, 10, 20]) torch.Size([242, 10, 1]) torch.Size([242, 10, 1])\ntorch.Size([242, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "dataset = load_NSEDataset('data_collection/pickled_datasets/itc_Jan17_w10_t20_p10_n10_o.pkl')\n",
    "train_dataloader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "keep_features = 'o'\n",
    "hidden_sz = 64\n",
    "model = NeuralNetwork(keep_features=keep_features, hidden_sz=hidden_sz).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "train(train_dataloader, model, loss_fn, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)\n",
    "# X = torch.rand(1, 28, 28, device=device)\n",
    "# logits = model(X)"
   ]
  }
 ]
}