{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from preprocessing.wrangling import get_indi_df, get_labels, slide_and_flatten\n",
    "from preprocessing.extract_features import get_all_ta_features, get_wavelet_coeffs\n",
    "from evaluation.eval import sliding_window_cv_regression, batch_test_swcv_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import datetime\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_closing_price(y, cls_price):\n",
    "    return y + cls_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistanceModel:\n",
    "    def __init__(self, persist_colname='Close'):\n",
    "        self.persist_colname = persist_colname\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PersistanceModel(persist_colname={})\".format(self.persist_colname)\n",
    "\n",
    "    def fit(self, Xtr, ytr):\n",
    "        pass\n",
    "\n",
    "    def predict(self, Xts):\n",
    "        return Xts.loc[:, self.persist_colname]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_cv_torch(swdf, y, model, optimizer, loss_fn, n_tr, n_ts=1, scorers=[], comment=\"\", post_processor=None):\n",
    "    assert len(swdf) == len(y), \"Length of X ([]) must match that of y ([]).\".format(len(X), len(y))\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    y_pred = []\n",
    "    y_target = []\n",
    "    agg_results = {}\n",
    "    if post_processor is not None:\n",
    "        post_processor_f, post_processor_args = post_processor[0], post_processor[1]\n",
    "        \n",
    "\n",
    "    for i_tr_start in range(0, len(swdf)-(n_tr+n_ts)):\n",
    "        # The last i_ts_end should be len(X).\n",
    "        # i_ts_end = i_ts_start + n_ts\n",
    "        # Now, i_tr_end = i_ts_start\n",
    "        # So, i_tr_start = i_ts_start - n_tr\n",
    "        # But, i_ts_start = i_ts_end - n_ts\n",
    "        # Thus, i_tr_start = i_ts_end - n_tr - n_ts\n",
    "        # Hence, last i_tr_start = len(X) - (n_tr + n_ts)\n",
    "\n",
    "        i_tr_end = i_ts_start = i_tr_start + n_tr \n",
    "        i_ts_end = i_ts_start + n_ts \n",
    "\n",
    "        Xtr, Xts = swdf[i_tr_start:i_tr_end, :, :], swdf[i_ts_start:i_ts_end, :, :]\n",
    "        ytr, yts = y[i_tr_start:i_tr_end].to_numpy(), y[i_ts_start:i_ts_end].to_numpy()\n",
    "        Xtr, Xts, ytr, yts = torch.Tensor(Xtr), torch.Tensor(Xts), torch.Tensor(ytr), torch.Tensor(yts)\n",
    "        Xtr, Xts, ytr, yts = Xtr.float().to(device), Xts.float().to(device), ytr.float().to(device), yts.float().to(device)\n",
    "        \n",
    "        model.to(device)\n",
    "        \n",
    "        epochs = 10\n",
    "        for e in range(epochs):\n",
    "            model.train()\n",
    "            pred = model(Xtr)\n",
    "            loss = loss_fn(pred, ytr)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(Xts)\n",
    "            mape = torch.mean((torch.abs((yts - pred) / yts)) * 100)\n",
    "            if device == \"cuda\":\n",
    "                pred = pred.detach()\n",
    "                yts = yts.detach()\n",
    "\n",
    "            if len(pred.shape) == 0:\n",
    "                pred = pred.unsqueeze(0)\n",
    "            if pred.shape[0] == 1:\n",
    "                y_pred.append(pred.item())\n",
    "                y_target.append(yts.item())\n",
    "            else:\n",
    "                y_pred, yts = list(y_pred), list(yts)\n",
    "                y_pred.extend(y_pred)\n",
    "                y_target.extend(yts)\n",
    "\n",
    "    # print(len(y_target), len(y_pred))\n",
    "    # print(y_pred, y_target)\n",
    "    if len(y_pred) > 1:\n",
    "        y_pred = np.squeeze(y_pred)\n",
    "\n",
    "    if post_processor is not None:\n",
    "        y_pred = post_processor_f(y_pred, **post_processor_args)\n",
    "        y_target = post_processor_f(y_target, **post_processor_args)\n",
    "\n",
    "    agg_results['time'] = datetime.datetime.now()\n",
    "    agg_results['model'] = str(model)\n",
    "    agg_results['comment'] = comment\n",
    "    for scorer in scorers:\n",
    "        agg_results[scorer.__name__] = scorer(y_target, y_pred)\n",
    "\n",
    "    return agg_results\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMBaseline, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.dense = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, swdf):\n",
    "        _, hncn = self.lstm(swdf)\n",
    "        hn, cn = hncn\n",
    "        hn = hn.squeeze()\n",
    "        op = self.dense(hn)\n",
    "        return op.squeeze()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n",
      "Using cpu device\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28702/2155921375.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 result = sliding_window_cv_torch(swdf10, cls_target10, model, optimizer, loss_fn, n_tr=60, n_ts=1, \n\u001b[0m\u001b[1;32m     44\u001b[0m                 scorers=[mean_squared_error,mean_absolute_percentage_error, r2_score], comment=\"\", post_processor=None)\n\u001b[1;32m     45\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28702/487202265.py\u001b[0m in \u001b[0;36msliding_window_cv_torch\u001b[0;34m(swdf, y, model, optimizer, loss_fn, n_tr, n_ts, scorers, comment, post_processor)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_dir = 'data_collection/stocks_list'\n",
    "list_prefix = \"ind_nifty\"\n",
    "list_suffix = \"list.csv\"\n",
    "save_dir = 'data_collection/ohlcv_data'\n",
    "save_prefix = \"ohlcv_\"\n",
    "save_suffix = \".csv\"\n",
    "resultfile = \"results/baseline_lstm.csv\"\n",
    "cap_n_stocks = 10\n",
    "results = []\n",
    "\n",
    "for f in os.listdir(list_dir):\n",
    "    if f.startswith(list_prefix) and f.endswith(list_suffix):\n",
    "            savefile = os.path.join(save_dir, save_prefix+f[9:-8]+save_suffix)\n",
    "            listfile = os.path.join(list_dir, f)\n",
    "            p = pd.read_csv(listfile)\n",
    "            symbols = list(p['Symbol'].values + '.NS')\n",
    "            if cap_n_stocks <= 0:\n",
    "                break\n",
    "            for symbol in symbols:\n",
    "                cap_n_stocks -= 1\n",
    "                if cap_n_stocks <= 0:\n",
    "                    break\n",
    "\n",
    "                df = get_indi_df(symbol, ohlcvfile=savefile, start_date=\"2017-01-01\")\n",
    "                # df = get_all_ta_features(df)\n",
    "                drop_columns = ['Date', 'Adj Close']\n",
    "                df.drop(drop_columns, axis=1, inplace=True)\n",
    "                move_dir_target, cls_target = get_labels(df['Close'])\n",
    "                df = df.iloc[:-1]\n",
    "                swdf10 = (sliding_window_view(df, (10, df.shape[1]))).squeeze()\n",
    "                cls_target10 = cls_target.iloc[(10 - 1):-1]\n",
    "                # df10 = slide_and_flatten(df, window_len=10)\n",
    "                # df10 = pd.DataFrame(df10, index=df.index[9:])\n",
    "                # df30 = slide_and_flatten(df, window_len=30)\n",
    "                # df30 = pd.DataFrame(df30, index=df.index[29:])\n",
    "                # df60 = slide_and_flatten(df, window_len=60)\n",
    "                # df60 = pd.DataFrame(df60, index=df.index[59:])\n",
    "                \n",
    "                # print(swdf10.shape, cls_target10.shape)\n",
    "                model = LSTMBaseline(swdf10.shape[2], 64)\n",
    "                optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1)\n",
    "                loss_fn = nn.MSELoss()\n",
    "                result = sliding_window_cv_torch(swdf10, cls_target10, model, optimizer, loss_fn, n_tr=60, n_ts=1, \n",
    "                scorers=[mean_squared_error,mean_absolute_percentage_error, r2_score], comment=\"\", post_processor=None)\n",
    "                results.append(result)\n",
    "                if resultfile is not None:\n",
    "                    file_exists = os.path.isfile(resultfile)\n",
    "    \n",
    "                    with open(resultfile, 'a', newline='') as f:\n",
    "                        writer = csv.DictWriter(f, fieldnames=results[0].keys(), delimiter=',', lineterminator='\\n')\n",
    "\n",
    "                        if not file_exists:\n",
    "                            writer.writeheader()  # file doesn't exist yet, write a header\n",
    "\n",
    "                        writer.writerows(results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d956a392b67227f401c129a901ef5c98887812674686cb9105f1c9b415cc849"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
